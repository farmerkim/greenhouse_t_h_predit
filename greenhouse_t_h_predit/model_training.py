import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import pickle
import os

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# ===== ì„¤ì • íŒŒë¼ë¯¸í„° =====
PREDICTION_HOURS = 6  # 6ì‹œê°„ í›„ ì˜ˆì¸¡
SEQUENCE_LENGTH = 6   # ê³¼ê±° 6ì‹œê°„ì˜ ê¸°ìƒ ë°ì´í„° ì‚¬ìš©
BATCH_SIZE = 64
EPOCHS = 150
LEARNING_RATE = 0.001
HIDDEN_SIZE = 128
NUM_LAYERS = 2
DROPOUT = 0.2

# ===== ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ =====
class GreenhouseDataset(Dataset):
    def __init__(self, X_weather, X_greenhouse, y):
        self.X_weather = torch.FloatTensor(X_weather)
        self.X_greenhouse = torch.FloatTensor(X_greenhouse)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return self.X_weather[idx], self.X_greenhouse[idx], self.y[idx]

# ===== LSTM ëª¨ë¸ ì •ì˜ =====
class GreenhousePredictionLSTM(nn.Module):
    """
    ê¸°ìƒì²­ ë°ì´í„° + í˜„ì¬ ì˜¨ì‹¤ ìƒíƒœ -> Nì‹œê°„ í›„ ì˜¨ì‹¤ ìƒíƒœ ì˜ˆì¸¡
    """
    def __init__(self, weather_input_size, greenhouse_input_size, 
                 hidden_size, num_layers, output_size, dropout=0.2):
        super(GreenhousePredictionLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # ê¸°ìƒ ë°ì´í„° ì²˜ë¦¬ LSTM
        self.weather_lstm = nn.LSTM(
            input_size=weather_input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # ì˜¨ì‹¤ ë°ì´í„° ì²˜ë¦¬ ë ˆì´ì–´
        self.greenhouse_encoder = nn.Sequential(
            nn.Linear(greenhouse_input_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # í†µí•© ë ˆì´ì–´
        combined_size = hidden_size + hidden_size // 2
        self.fc = nn.Sequential(
            nn.Linear(combined_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )
    
    def forward(self, weather_data, greenhouse_data):
        # ê¸°ìƒ ë°ì´í„° LSTM ì²˜ë¦¬
        lstm_out, (h_n, c_n) = self.weather_lstm(weather_data)
        weather_features = lstm_out[:, -1, :]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
        
        # ì˜¨ì‹¤ ë°ì´í„° ì¸ì½”ë”©
        greenhouse_features = self.greenhouse_encoder(greenhouse_data)
        
        # íŠ¹ì„± ê²°í•©
        combined = torch.cat([weather_features, greenhouse_features], dim=1)
        
        # ìµœì¢… ì˜ˆì¸¡
        output = self.fc(combined)
        return output

# ===== ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ =====
def prepare_data(df, prediction_hours=6, sequence_length=6):
    """
    í˜„ì¬ ì‹œì ì˜ (ê¸°ìƒ + ì˜¨ì‹¤) ë°ì´í„°ë¡œ Nì‹œê°„ í›„ ì˜¨ì‹¤ ìƒíƒœ ì˜ˆì¸¡
    """
    print(f"\në°ì´í„° ì¤€ë¹„: {prediction_hours}ì‹œê°„ í›„ ì˜ˆì¸¡ ëª¨ë¸")
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}ì‹œê°„")
    
    # ì»¬ëŸ¼ ë¶„ë¥˜
    # ê¸°ìƒ ë°ì´í„°: outer_ë¡œ ì‹œì‘í•˜ê±°ë‚˜ wind, rainfall, solar_rad, pressure
    weather_cols = [col for col in df.columns if col.startswith('outer_') or 
                    col in ['wind_speed', 'wind_dir', 'rainfall', 'solar_rad', 'pressure']]
    
    # ì‹œê°„ íŠ¹ì„±
    time_cols = [col for col in df.columns if col in ['hour', 'day', 'month', 'dayofweek', 
                 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 
                 'day_sin', 'day_cos', 'season', 'is_weekend']]
    
    # ì˜¨ì‹¤ lag íŠ¹ì„± (ê³¼ê±° ì˜¨ì‹¤ ìƒíƒœ)
    lag_cols = [col for col in df.columns if 'lag' in col]
    
    # ê¸°ìƒ ë°ì´í„°ì— ì‹œê°„ íŠ¹ì„± ì¶”ê°€
    weather_cols = weather_cols + time_cols
    
    # ì˜¨ì‹¤ ë°ì´í„°ëŠ” lag + ì‹œê°„ íŠ¹ì„±
    greenhouse_cols = lag_cols + time_cols
    
    print(f"\nì…ë ¥ íŠ¹ì„±:")
    print(f"  - ê¸°ìƒ ë°ì´í„°: {len(weather_cols)}ê°œ")
    print(f"    ì˜ˆì‹œ: {weather_cols[:5]}")
    print(f"  - ì˜¨ì‹¤ ë°ì´í„°: {len(greenhouse_cols)}ê°œ")
    print(f"    ì˜ˆì‹œ: {greenhouse_cols[:5]}")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ (ì˜¨ì‹¤ ì˜¨ë„, ìŠµë„)
    target_cols = ['inner_temp', 'inner_hum']
    print(f"\nì¶œë ¥ íƒ€ê²Ÿ: {target_cols}")
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    X_weather_list = []
    X_greenhouse_list = []
    y_list = []
    
    for i in range(len(df) - sequence_length - prediction_hours + 1):
        # ì…ë ¥: ê³¼ê±° sequence_length ì‹œê°„ì˜ ê¸°ìƒ ë°ì´í„°
        weather_seq = df[weather_cols].iloc[i:i+sequence_length].values
        
        # ì…ë ¥: í˜„ì¬ ì‹œì ì˜ ì˜¨ì‹¤ ìƒíƒœ (lag + ì‹œê°„)
        greenhouse_current = df[greenhouse_cols].iloc[i+sequence_length-1].values
        
        # ì¶œë ¥: prediction_hours ì‹œê°„ í›„ì˜ ì˜¨ì‹¤ ì˜¨ìŠµë„
        target_future = df[target_cols].iloc[i+sequence_length+prediction_hours-1].values
        
        X_weather_list.append(weather_seq)
        X_greenhouse_list.append(greenhouse_current)
        y_list.append(target_future)
    
    X_weather = np.array(X_weather_list)
    X_greenhouse = np.array(X_greenhouse_list)
    y = np.array(y_list)
    
    print(f"\nìƒì„±ëœ ë°ì´í„°:")
    print(f"  X_weather: {X_weather.shape} (ìƒ˜í”Œ, ì‹œí€€ìŠ¤, ê¸°ìƒíŠ¹ì„±)")
    print(f"  X_greenhouse: {X_greenhouse.shape} (ìƒ˜í”Œ, ì˜¨ì‹¤íŠ¹ì„±)")
    print(f"  y: {y.shape} (ìƒ˜í”Œ, íƒ€ê²Ÿ)")
    print(f"  ì´ ìƒ˜í”Œ: {len(y):,}ê°œ")
    
    return X_weather, X_greenhouse, y, weather_cols, greenhouse_cols, target_cols

# ===== í•™ìŠµ í•¨ìˆ˜ =====
def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    
    for X_weather, X_greenhouse, y in dataloader:
        X_weather = X_weather.to(device)
        X_greenhouse = X_greenhouse.to(device)
        y = y.to(device)
        
        # Forward
        outputs = model(X_weather, X_greenhouse)
        loss = criterion(outputs, y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)

# ===== ê²€ì¦ í•¨ìˆ˜ =====
def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for X_weather, X_greenhouse, y in dataloader:
            X_weather = X_weather.to(device)
            X_greenhouse = X_greenhouse.to(device)
            y = y.to(device)
            
            outputs = model(X_weather, X_greenhouse)
            loss = criterion(outputs, y)
            
            total_loss += loss.item()
            predictions.append(outputs.cpu().numpy())
            actuals.append(y.cpu().numpy())
    
    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)
    
    return total_loss / len(dataloader), predictions, actuals

# ===== ë©”ì¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ =====
def main():
    print("=" * 70)
    print("ì˜¨ì‹¤ ì˜¨ìŠµë„ ì˜ˆì¸¡ LSTM ëª¨ë¸ í•™ìŠµ")
    print("=" * 70)
    print(f"í•™ìŠµ ë°ì´í„°: 4/16-10/27 ê¸°ìƒì²­ ASOS + ì˜¨ì‹¤ ë°ì´í„°")
    print(f"ì˜ˆì¸¡ ëª©í‘œ: {PREDICTION_HOURS}ì‹œê°„ í›„ ì˜¨ì‹¤ ì˜¨ë„/ìŠµë„")
    print("=" * 70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[1/8] ë°ì´í„° ë¡œë”©...")
    df = pd.read_csv('output/preprocessed_data.csv')
    
    if 'Date&Time' in df.columns:
        print(f"ë°ì´í„° ê¸°ê°„: {df['Date&Time'].min()} ~ {df['Date&Time'].max()}")
        df = df.drop(columns=['Date&Time'])
    
    print(f"ë°ì´í„° shape: {df.shape}")
    print(f"ì´ ë ˆì½”ë“œ: {len(df):,}ê°œ")
    
    # 2. ë°ì´í„° ì¤€ë¹„
    print("\n[2/8] ë°ì´í„° ì¤€ë¹„...")
    X_weather, X_greenhouse, y, weather_cols, greenhouse_cols, target_cols = \
        prepare_data(df, PREDICTION_HOURS, SEQUENCE_LENGTH)
    
    # 3. ë°ì´í„° ë¶„í•  (Train/Val)
    print("\n[3/8] í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  (80:20)...")
    
    # ì‹œê³„ì—´ì´ë¯€ë¡œ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¶„í• 
    split_idx = int(len(y) * 0.8)
    
    X_weather_train = X_weather[:split_idx]
    X_weather_val = X_weather[split_idx:]
    
    X_greenhouse_train = X_greenhouse[:split_idx]
    X_greenhouse_val = X_greenhouse[split_idx:]
    
    y_train = y[:split_idx]
    y_val = y[split_idx:]
    
    print(f"Train ìƒ˜í”Œ: {len(y_train):,}ê°œ")
    print(f"Val ìƒ˜í”Œ: {len(y_val):,}ê°œ")
    
    # 4. ìŠ¤ì¼€ì¼ë§ (Train ë°ì´í„°ë¡œë§Œ fit)
    print("\n[4/8] ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
    
    # ê¸°ìƒ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler_weather = StandardScaler()
    X_weather_train_2d = X_weather_train.reshape(-1, X_weather_train.shape[-1])
    scaler_weather.fit(X_weather_train_2d)
    
    X_weather_train_scaled = scaler_weather.transform(X_weather_train_2d).reshape(X_weather_train.shape)
    X_weather_val_2d = X_weather_val.reshape(-1, X_weather_val.shape[-1])
    X_weather_val_scaled = scaler_weather.transform(X_weather_val_2d).reshape(X_weather_val.shape)
    
    # ì˜¨ì‹¤ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler_greenhouse = StandardScaler()
    X_greenhouse_train_scaled = scaler_greenhouse.fit_transform(X_greenhouse_train)
    X_greenhouse_val_scaled = scaler_greenhouse.transform(X_greenhouse_val)
    
    # íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ë§
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train)
    y_val_scaled = scaler_y.transform(y_val)
    
    print("âœ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ (Train ë°ì´í„°ë¡œë§Œ fit)")
    
    # 5. ë°ì´í„°ë¡œë” ìƒì„±
    print("\n[5/8] ë°ì´í„°ë¡œë” ìƒì„±...")
    train_dataset = GreenhouseDataset(X_weather_train_scaled, X_greenhouse_train_scaled, y_train_scaled)
    val_dataset = GreenhouseDataset(X_weather_val_scaled, X_greenhouse_val_scaled, y_val_scaled)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # 6. ëª¨ë¸ ì´ˆê¸°í™”
    print("\n[6/8] ëª¨ë¸ ì´ˆê¸°í™”...")
    
    weather_input_size = X_weather.shape[2]
    greenhouse_input_size = X_greenhouse.shape[1]
    output_size = len(target_cols)
    
    model = GreenhousePredictionLSTM(
        weather_input_size=weather_input_size,
        greenhouse_input_size=greenhouse_input_size,
        hidden_size=HIDDEN_SIZE,
        num_layers=NUM_LAYERS,
        output_size=output_size,
        dropout=DROPOUT
    ).to(device)
    
    print(f"\nëª¨ë¸ êµ¬ì¡°:")
    print(f"  - ê¸°ìƒ ì…ë ¥ í¬ê¸°: {weather_input_size}")
    print(f"  - ì˜¨ì‹¤ ì…ë ¥ í¬ê¸°: {greenhouse_input_size}")
    print(f"  - Hidden size: {HIDDEN_SIZE}")
    print(f"  - LSTM layers: {NUM_LAYERS}")
    print(f"  - ì¶œë ¥ í¬ê¸°: {output_size}")
    print(f"  - ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}ê°œ")
    
    # 7. ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    print("\n[7/8] ì˜µí‹°ë§ˆì´ì € ì„¤ì •...")
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=True
    )
    
    # 8. í•™ìŠµ
    print(f"\n[8/8] ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({EPOCHS} epochs)...")
    print("-" * 70)
    
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 25
    
    train_losses = []
    val_losses = []
    
    for epoch in range(EPOCHS):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_preds, val_actuals = validate(model, val_loader, criterion, device)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        scheduler.step(val_loss)
        
        # ë¡œê·¸ ì¶œë ¥
        if (epoch + 1) % 5 == 0 or epoch == 0:
            # ì‹¤ì œ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•˜ì—¬ ë©”íŠ¸ë¦­ ê³„ì‚°
            val_preds_real = scaler_y.inverse_transform(val_preds)
            val_actuals_real = scaler_y.inverse_transform(val_actuals)
            
            # MAE
            mae_temp = np.mean(np.abs(val_preds_real[:, 0] - val_actuals_real[:, 0]))
            mae_hum = np.mean(np.abs(val_preds_real[:, 1] - val_actuals_real[:, 1]))
            
            # RMSE
            rmse_temp = np.sqrt(np.mean((val_preds_real[:, 0] - val_actuals_real[:, 0])**2))
            rmse_hum = np.sqrt(np.mean((val_preds_real[:, 1] - val_actuals_real[:, 1])**2))
            
            # RÂ² Score
            r2_temp = r2_score(val_actuals_real[:, 0], val_preds_real[:, 0])
            r2_hum = r2_score(val_actuals_real[:, 1], val_preds_real[:, 1])
            
            print(f"Epoch [{epoch+1:3d}/{EPOCHS}] Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")
            print(f"  MAE  â†’ ì˜¨ë„: {mae_temp:.2f}Â°C, ìŠµë„: {mae_hum:.2f}%")
            print(f"  RMSE â†’ ì˜¨ë„: {rmse_temp:.2f}Â°C, ìŠµë„: {rmse_hum:.2f}%")
            print(f"  RÂ²   â†’ ì˜¨ë„: {r2_temp:.3f}, ìŠµë„: {r2_hum:.3f}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'config': {
                    'weather_input_size': weather_input_size,
                    'greenhouse_input_size': greenhouse_input_size,
                    'hidden_size': HIDDEN_SIZE,
                    'num_layers': NUM_LAYERS,
                    'output_size': output_size,
                    'dropout': DROPOUT,
                    'prediction_hours': PREDICTION_HOURS,
                    'sequence_length': SEQUENCE_LENGTH,
                    'weather_cols': weather_cols,
                    'greenhouse_cols': greenhouse_cols,
                    'target_cols': target_cols
                }
            }, 'output/best_model.pth')
            
            if (epoch + 1) % 5 == 0:
                print(f"  âœ“ Best model saved! (Val Loss: {val_loss:.6f})")
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= early_stop_patience:
            print(f"\nâš  Early stopping at epoch {epoch+1}")
            break
    
    # 9. ìŠ¤ì¼€ì¼ëŸ¬ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
    print("\n[9/9] ìŠ¤ì¼€ì¼ëŸ¬ ë° ë©”íƒ€ë°ì´í„° ì €ì¥...")
    os.makedirs('output/cache', exist_ok=True)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    with open('output/cache/scaler_weather.pkl', 'wb') as f:
        pickle.dump(scaler_weather, f)
    with open('output/cache/scaler_greenhouse.pkl', 'wb') as f:
        pickle.dump(scaler_greenhouse, f)
    with open('output/cache/scaler_y.pkl', 'wb') as f:
        pickle.dump(scaler_y, f)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'prediction_hours': PREDICTION_HOURS,
        'sequence_length': SEQUENCE_LENGTH,
        'weather_cols': weather_cols,
        'greenhouse_cols': greenhouse_cols,
        'target_cols': target_cols,
        'model_config': {
            'hidden_size': HIDDEN_SIZE,
            'num_layers': NUM_LAYERS,
            'dropout': DROPOUT
        },
        'training_info': {
            'train_samples': len(y_train),
            'val_samples': len(y_val),
            'best_val_loss': float(best_val_loss)
        }
    }
    with open('output/cache/metadata.pkl', 'wb') as f:
        pickle.dump(metadata, f)
    
    # í•™ìŠµ ì´ë ¥ ì €ì¥
    results = pd.DataFrame({
        'epoch': range(1, len(train_losses) + 1),
        'train_loss': train_losses,
        'val_loss': val_losses
    })
    results.to_csv('output/training_history.csv', index=False)
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 70)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 70)
    print(f"ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
    print(f"ì˜ˆì¸¡ ì‹œê°„: {PREDICTION_HOURS}ì‹œê°„ í›„")
    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {SEQUENCE_LENGTH}ì‹œê°„")
    print(f"\nì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“¦ ëª¨ë¸: output/best_model.pth")
    print(f"  ğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬: output/cache/scaler_*.pkl")
    print(f"  ğŸ“‹ ë©”íƒ€ë°ì´í„°: output/cache/metadata.pkl")
    print(f"  ğŸ“ˆ í•™ìŠµ ì´ë ¥: output/training_history.csv")
    print("=" * 70)
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: future_prediction.pyë¡œ ë‹¨ê¸°ì˜ˆë³´ ë°ì´í„° ì˜ˆì¸¡")

if __name__ == "__main__":
    main()